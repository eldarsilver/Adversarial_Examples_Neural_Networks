{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D\n",
    "\n",
    "\n",
    "class _ArgsWrapper(object):\n",
    "    \"\"\"\n",
    "    Wrapper that allows attribute access to dictionaries\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        if not isinstance(args, dict):\n",
    "            args = vars(args)\n",
    "        self.args = args\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return self.args.get(name)\n",
    "\n",
    "\n",
    "def save_model(model, dir, filename, weights_only=False):\n",
    "    \"\"\"\n",
    "    Save Keras model\n",
    "    :param model:\n",
    "    :param dir:\n",
    "    :param filename:\n",
    "    :param weights_only:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # If target directory does not exist, create\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    # Construct full path\n",
    "    filepath = os.path.join(dir, filename)\n",
    "\n",
    "    if weights_only:\n",
    "        # Dump model weights\n",
    "        model.save_weights(filepath)\n",
    "        print(\"Model weights were saved to: \" + filepath)\n",
    "    else:\n",
    "        # Dump model architecture and weights\n",
    "        model.save(filepath)\n",
    "        print(\"Model was saved to: \" + filepath)\n",
    "\n",
    "\n",
    "def load_model(directory, filename, weights_only=False, model=None):\n",
    "    \"\"\"\n",
    "    Loads Keras model\n",
    "    :param directory:\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # If restoring model weights only, make sure model argument was given\n",
    "    if weights_only:\n",
    "        assert model is not None\n",
    "\n",
    "    # Construct full path to dumped model\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    # Check if file exists\n",
    "    assert os.path.exists(filepath)\n",
    "\n",
    "    # Return Keras model\n",
    "    if weights_only:\n",
    "        result = model.load_weights(filepath)\n",
    "        print(result)\n",
    "        return model.load_weights(filepath)\n",
    "    else:\n",
    "        return keras.models.load_model(filepath)\n",
    "\n",
    "\n",
    "def batch_indices(batch_nb, data_length, batch_size):\n",
    "    \"\"\"\n",
    "    This helper function computes a batch start and end index\n",
    "    :param batch_nb: the batch number\n",
    "    :param data_length: the total length of the data being parsed by batches\n",
    "    :param batch_size: the number of inputs in each batch\n",
    "    :return: pair of (start, end) indices\n",
    "    \"\"\"\n",
    "    # Batch start and end index\n",
    "    start = int(batch_nb * batch_size)\n",
    "    end = int((batch_nb + 1) * batch_size)\n",
    "\n",
    "    # When there are not enough inputs left, we reuse some to complete the\n",
    "    # batch\n",
    "    if end > data_length:\n",
    "        shift = end - data_length\n",
    "        start -= shift\n",
    "        end -= shift\n",
    "\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def other_classes(nb_classes, class_ind):\n",
    "    \"\"\"\n",
    "    Heper function that returns a list of class indices without one class\n",
    "    :param nb_classes: number of classes in total\n",
    "    :param class_ind: the class index to be omitted\n",
    "    :return: list of class indices without one class\n",
    "    \"\"\"\n",
    "\n",
    "    other_classes_list = list(range(nb_classes))\n",
    "    other_classes_list.remove(class_ind)\n",
    "\n",
    "    return other_classes_list\n",
    "\n",
    "\n",
    "def cnn_model(logits=False, input_ph=None, img_rows=28, img_cols=28,\n",
    "              channels=1, nb_filters=64, nb_classes=10):\n",
    "    \"\"\"\n",
    "    Defines a CNN model using Keras sequential model\n",
    "    :param logits: If set to False, returns a Keras model, otherwise will also\n",
    "                    return logits tensor\n",
    "    :param input_ph: The TensorFlow tensor for the input\n",
    "                    (needed if returning logits)\n",
    "                    (\"ph\" stands for placeholder but it need not actually be a\n",
    "                    placeholder)\n",
    "    :param img_rows: number of row in the image\n",
    "    :param img_cols: number of columns in the image\n",
    "    :param channels: number of color channels (e.g., 1 for MNIST)\n",
    "    :param nb_filters: number of convolutional filters per layer\n",
    "    :param nb_classes: the number of output classes\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    if keras.backend.image_dim_ordering() == 'th':\n",
    "        input_shape = (channels, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "    layers = [Dropout(0.2, input_shape=input_shape),\n",
    "              Convolution2D(nb_filters, 8, 8,\n",
    "                            subsample=(2, 2),\n",
    "                            border_mode=\"same\"\n",
    "                            ),\n",
    "              Activation('relu'),\n",
    "              Convolution2D(nb_filters * 2, 6, 6, subsample=(2, 2),\n",
    "                            border_mode=\"valid\"),\n",
    "              Activation('relu'),\n",
    "              Convolution2D(nb_filters * 2, 5, 5, subsample=(1, 1)),\n",
    "              Activation('relu'),\n",
    "              Dropout(0.5),\n",
    "              Flatten(),\n",
    "              Dense(nb_classes)]\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    if logits:\n",
    "        logits_tensor = model(input_ph)\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    if logits:\n",
    "        return model, logits_tensor\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "def pair_visual(original, adversarial, figure=None):\n",
    "    \"\"\"\n",
    "    This function displays two images: the original and the adversarial sample\n",
    "    :param original: the original input\n",
    "    :param adversarial: the input after perterbations have been applied\n",
    "    :param figure: if we've already displayed images, use the same plot\n",
    "    :return: the matplot figure to reuse for future samples\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure our inputs are of proper shape\n",
    "    assert(len(original.shape) == 2 or len(original.shape) == 3)\n",
    "\n",
    "    # To avoid creating figures per input sample, reuse the sample plot\n",
    "    if figure is None:\n",
    "        plt.ion()\n",
    "        figure = plt.figure()\n",
    "        figure.canvas.set_window_title('Cleverhans: Pair Visualization')\n",
    "\n",
    "    # Add the images to the plot\n",
    "    perterbations = adversarial - original\n",
    "    for index, image in enumerate((original, perterbations, adversarial)):\n",
    "        figure.add_subplot(1, 3, index + 1)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # If the image is 2D, then we have 1 color channel\n",
    "        if len(image.shape) == 2:\n",
    "            plt.imshow(image, cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(image)\n",
    "\n",
    "        # Give the plot some time to update\n",
    "        plt.pause(0.01)\n",
    "\n",
    "    # Draw the plot and return\n",
    "    plt.show()\n",
    "    return figure\n",
    "\n",
    "\n",
    "def grid_visual(data):\n",
    "    \"\"\"\n",
    "    This function displays a grid of images to show full misclassification\n",
    "    :param data: grid data of the form;\n",
    "        [nb_classes : nb_classes : img_rows : img_cols : nb_channels]\n",
    "    :return: if necessary, the matplot figure to reuse\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure interactive mode is disabled and initialize our graph\n",
    "    plt.ioff()\n",
    "    figure = plt.figure()\n",
    "    figure.canvas.set_window_title('Cleverhans: Grid Visualization')\n",
    "\n",
    "    # Add the images to the plot\n",
    "    num_cols = data.shape[0]\n",
    "    num_rows = data.shape[1]\n",
    "    num_channels = data.shape[4]\n",
    "    current_row = 0\n",
    "    for y in xrange(num_rows):\n",
    "        for x in xrange(num_cols):\n",
    "            figure.add_subplot(num_cols, num_rows, (x+1)+(y*num_rows))\n",
    "            plt.axis('off')\n",
    "\n",
    "            if num_channels == 1:\n",
    "                plt.imshow(data[x, y, :, :, 0], cmap='gray')\n",
    "            else:\n",
    "                plt.imshow(data[x, y, :, :, :])\n",
    "\n",
    "    # Draw the plot and return\n",
    "    plt.show()\n",
    "    return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import keras\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "class _FlagsWrapper(_ArgsWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper that tries to find missing parameters in TensorFlow FLAGS\n",
    "    for backwards compatibility.\n",
    "\n",
    "    Plain _ArgsWrapper should be used instead if the support for FLAGS\n",
    "    is removed.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        val = self.args.get(name)\n",
    "        if val is None:\n",
    "            warnings.warn('Setting parameters ({}) from TensorFlow FLAGS is '\n",
    "                          'deprecated.'.format(name))\n",
    "            val = FLAGS.__getattr__(name)\n",
    "        return val\n",
    "\n",
    "\n",
    "def model_loss(y, model, mean=True):\n",
    "    \"\"\"\n",
    "    Define loss of TF graph\n",
    "    :param y: correct labels\n",
    "    :param model: output of the model\n",
    "    :param mean: boolean indicating whether should return mean of loss\n",
    "                 or vector of losses for each input of the batch\n",
    "    :return: return mean of loss if True, otherwise return vector with per\n",
    "             sample loss\n",
    "    \"\"\"\n",
    "\n",
    "    op = model.op\n",
    "    if \"softmax\" in str(op).lower():\n",
    "        logits, = op.inputs\n",
    "    else:\n",
    "        logits = model\n",
    "\n",
    "    out = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "\n",
    "    if mean:\n",
    "        out = tf.reduce_mean(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def tf_model_train(*args, **kwargs):\n",
    "    warnings.warn(\"`tf_model_train` is deprecated. Switch to `model_train`.\"\n",
    "                  \"`tf_model_train` will be removed after 2017-07-18.\")\n",
    "    return model_train(*args, **kwargs)\n",
    "\n",
    "\n",
    "def model_train(sess, x, y, predictions, X_train, Y_train, save=False,\n",
    "                predictions_adv=None, evaluate=None, verbose=True, args=None):\n",
    "    \"\"\"\n",
    "    Train a TF graph\n",
    "    :param sess: TF session to use when training the graph\n",
    "    :param x: input placeholder\n",
    "    :param y: output placeholder (for labels)\n",
    "    :param predictions: model output predictions\n",
    "    :param X_train: numpy array with training inputs\n",
    "    :param Y_train: numpy array with training outputs\n",
    "    :param save: boolean controling the save operation\n",
    "    :param predictions_adv: if set with the adversarial example tensor,\n",
    "                            will run adversarial training\n",
    "    :param args: dict or argparse `Namespace` object.\n",
    "                 Should contain `nb_epochs`, `learning_rate`,\n",
    "                 `batch_size`\n",
    "                 If save is True, should also contain 'train_dir'\n",
    "                 and 'filename'\n",
    "    :return: True if model trained\n",
    "    \"\"\"\n",
    "    args = _FlagsWrapper(args or {})\n",
    "\n",
    "    # Check that necessary arguments were given (see doc above)\n",
    "    assert args.nb_epochs, \"Number of epochs was not given in args dict\"\n",
    "    assert args.learning_rate, \"Learning rate was not given in args dict\"\n",
    "    assert args.batch_size, \"Batch size was not given in args dict\"\n",
    "\n",
    "    if save:\n",
    "        assert args.train_dir, \"Directory for save was not given in args dict\"\n",
    "        assert args.filename, \"Filename for save was not given in args dict\"\n",
    "\n",
    "    # Define loss\n",
    "    loss = model_loss(y, predictions)\n",
    "    if predictions_adv is not None:\n",
    "        loss = (loss + model_loss(y, predictions_adv)) / 2\n",
    "\n",
    "    train_step = tf.train.AdadeltaOptimizer(learning_rate=args.learning_rate,\n",
    "                                            rho=0.95,\n",
    "                                            epsilon=1e-08).minimize(loss)\n",
    "\n",
    "    with sess.as_default():\n",
    "        tf.global_variables_initializer().run()\n",
    "        \"\"\"if hasattr(tf, \"global_variables_initializer\"):\n",
    "            tf.global_variables_initializer().run()\n",
    "        else:\n",
    "            warnings.warn(\"Update your copy of tensorflow; future versions of \"\n",
    "                          \"cleverhans may drop support for this version.\")\n",
    "            sess.run(tf.initialize_all_variables())\"\"\"\n",
    "\n",
    "        for epoch in six.moves.xrange(args.nb_epochs):\n",
    "            if verbose:\n",
    "                print(\"Epoch \" + str(epoch))\n",
    "\n",
    "            # Compute number of batches\n",
    "            nb_batches = int(math.ceil(float(len(X_train)) / args.batch_size))\n",
    "            assert nb_batches * args.batch_size >= len(X_train)\n",
    "\n",
    "            prev = time.time()\n",
    "            for batch in range(nb_batches):\n",
    "\n",
    "                # Compute batch start and end indices\n",
    "                start, end = batch_indices(\n",
    "                    batch, len(X_train), args.batch_size)\n",
    "\n",
    "                # Perform one training step\n",
    "                train_step.run(feed_dict={x: X_train[start:end],\n",
    "                                          y: Y_train[start:end],\n",
    "                                          keras.backend.learning_phase(): 1})\n",
    "            assert end >= len(X_train)  # Check that all examples were used\n",
    "            cur = time.time()\n",
    "            if verbose:\n",
    "                print(\"\\tEpoch took \" + str(cur - prev) + \" seconds\")\n",
    "            prev = cur\n",
    "            if evaluate is not None:\n",
    "                evaluate()\n",
    "\n",
    "        if save:\n",
    "            save_path = os.path.join(args.train_dir, args.filename)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, save_path)\n",
    "            print(\"Completed model training and model saved at:\"\n",
    "                  + str(save_path))\n",
    "        else:\n",
    "            print(\"Completed model training.\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def tf_model_eval(*args, **kwargs):\n",
    "    warnings.warn(\"`tf_model_eval` is deprecated. Switch to `model_eval`.\"\n",
    "                  \"`tf_model_eval` will be removed after 2017-07-18.\")\n",
    "    return model_eval(*args, **kwargs)\n",
    "\n",
    "\n",
    "def model_eval(sess, x, y, model, X_test, Y_test, args=None):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of a TF model on some data\n",
    "    :param sess: TF session to use when training the graph\n",
    "    :param x: input placeholder\n",
    "    :param y: output placeholder (for labels)\n",
    "    :param model: model output predictions\n",
    "    :param X_test: numpy array with training inputs\n",
    "    :param Y_test: numpy array with training outputs\n",
    "    :param args: dict or argparse `Namespace` object.\n",
    "                 Should contain `batch_size`\n",
    "    :return: a float with the accuracy value\n",
    "    \"\"\"\n",
    "    args = _FlagsWrapper(args or {})\n",
    "\n",
    "    assert args.batch_size, \"Batch size was not given in args dict\"\n",
    "\n",
    "    # Define symbol for accuracy\n",
    "    acc_value = keras.metrics.categorical_accuracy(y, model)\n",
    "\n",
    "    # Init result var\n",
    "    accuracy = 0.0\n",
    "\n",
    "    with sess.as_default():\n",
    "        # Compute number of batches\n",
    "        nb_batches = int(math.ceil(float(len(X_test)) / args.batch_size))\n",
    "        assert nb_batches * args.batch_size >= len(X_test)\n",
    "\n",
    "        for batch in range(nb_batches):\n",
    "            if batch % 100 == 0 and batch > 0:\n",
    "                print(\"Batch \" + str(batch))\n",
    "\n",
    "            # Must not use the `batch_indices` function here, because it\n",
    "            # repeats some examples.\n",
    "            # It's acceptable to repeat during training, but not eval.\n",
    "            start = batch * args.batch_size\n",
    "            end = min(len(X_test), start + args.batch_size)\n",
    "            cur_batch_size = end - start\n",
    "\n",
    "            # The last batch may be smaller than all others, so we need to\n",
    "            # account for variable batch size here\n",
    "            accuracy += cur_batch_size * acc_value.eval(\n",
    "                feed_dict={x: X_test[start:end],\n",
    "                           y: Y_test[start:end],\n",
    "                           keras.backend.learning_phase(): 0})\n",
    "        assert end >= len(X_test)\n",
    "\n",
    "        # Divide by number of examples to get final value\n",
    "        accuracy /= len(X_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def tf_model_load(sess):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sess:\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with sess.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, os.path.join(FLAGS.train_dir, FLAGS.filename))\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def batch_eval(sess, tf_inputs, tf_outputs, numpy_inputs, args=None):\n",
    "    \"\"\"\n",
    "    A helper function that computes a tensor on numpy inputs by batches.\n",
    "\n",
    "    :param sess:\n",
    "    :param tf_inputs:\n",
    "    :param tf_outputs:\n",
    "    :param numpy_inputs:\n",
    "    :param args: dict or argparse `Namespace` object.\n",
    "                 Should contain `batch_size`\n",
    "    \"\"\"\n",
    "    args = _FlagsWrapper(args or {})\n",
    "\n",
    "    assert args.batch_size, \"Batch size was not given in args dict\"\n",
    "\n",
    "    n = len(numpy_inputs)\n",
    "    assert n > 0\n",
    "    assert n == len(tf_inputs)\n",
    "    m = numpy_inputs[0].shape[0]\n",
    "    for i in six.moves.xrange(1, n):\n",
    "        assert numpy_inputs[i].shape[0] == m\n",
    "    out = []\n",
    "    for _ in tf_outputs:\n",
    "        out.append([])\n",
    "    with sess.as_default():\n",
    "        for start in six.moves.xrange(0, m, args.batch_size):\n",
    "            batch = start // args.batch_size\n",
    "            if batch % 100 == 0 and batch > 0:\n",
    "                print(\"Batch \" + str(batch))\n",
    "\n",
    "            # Compute batch start and end indices\n",
    "            start = batch * args.batch_size\n",
    "            end = start + args.batch_size\n",
    "            numpy_input_batches = [numpy_input[start:end]\n",
    "                                   for numpy_input in numpy_inputs]\n",
    "            cur_batch_size = numpy_input_batches[0].shape[0]\n",
    "            assert cur_batch_size <= args.batch_size\n",
    "            for e in numpy_input_batches:\n",
    "                assert e.shape[0] == cur_batch_size\n",
    "\n",
    "            feed_dict = dict(zip(tf_inputs, numpy_input_batches))\n",
    "            feed_dict[keras.backend.learning_phase()] = 0\n",
    "            numpy_output_batches = sess.run(tf_outputs, feed_dict=feed_dict)\n",
    "            for e in numpy_output_batches:\n",
    "                assert e.shape[0] == cur_batch_size, e.shape\n",
    "            for out_elem, numpy_output_batch in zip(out, numpy_output_batches):\n",
    "                out_elem.append(numpy_output_batch)\n",
    "\n",
    "    out = [np.concatenate(x, axis=0) for x in out]\n",
    "    for e in out:\n",
    "        assert e.shape[0] == m, e.shape\n",
    "    return out\n",
    "\n",
    "\n",
    "def model_argmax(sess, x, predictions, sample):\n",
    "    \"\"\"\n",
    "    Helper function that computes the current class prediction\n",
    "    :param sess: TF session\n",
    "    :param x: the input placeholder\n",
    "    :param predictions: the model's symbolic output\n",
    "    :param sample: (1 x 1 x img_rows x img_cols) numpy array with sample input\n",
    "    :return: the argmax output of predictions, i.e. the current predicted class\n",
    "    \"\"\"\n",
    "\n",
    "    feed_dict = {x: sample, keras.backend.learning_phase(): 0}\n",
    "    probabilities = sess.run(predictions, feed_dict)\n",
    "\n",
    "    return np.argmax(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import warnings\n",
    "\n",
    "\n",
    "def data_mnist():\n",
    "    \"\"\"\n",
    "    Preprocess MNIST dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # These values are specific to MNIST\n",
    "    img_rows = 28\n",
    "    img_cols = 28\n",
    "    nb_classes = 10\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    if keras.backend.image_dim_ordering() == 'th':\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "        X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    else:\n",
    "        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def model_mnist(logits=False, input_ph=None, img_rows=28, img_cols=28,\n",
    "                nb_filters=64, nb_classes=10):\n",
    "    warnings.warn(\"`utils_mnist.model_mnist` is deprecated. Switch to\"\n",
    "                  \"`utils.cnn_model`. `utils_mnist.model_mnist` will \"\n",
    "                  \"be removed after 2017-08-17.\")\n",
    "    return utils.cnn_model(logits=logits, input_ph=input_ph,\n",
    "                           img_rows=img_rows, img_cols=img_cols,\n",
    "                           nb_filters=nb_filters, nb_classes=nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(x, predictions, eps, clip_min=None, clip_max=None):\n",
    "    \"\"\"\n",
    "    TensorFlow implementation of the Fast Gradient\n",
    "    Sign method.\n",
    "    :param x: the input placeholder\n",
    "    :param predictions: the model's output tensor\n",
    "    :param eps: the epsilon (input variation parameter)\n",
    "    :param clip_min: optional parameter that can be used to set a minimum\n",
    "                    value for components of the example returned\n",
    "    :param clip_max: optional parameter that can be used to set a maximum\n",
    "                    value for components of the example returned\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute loss\n",
    "    #print(\"PREDICTIONS\")\n",
    "    #print(predictions.eval())\n",
    "    #print(tf.reduce_max(predictions, 1, keep_dims=True))\n",
    "    y = tf.to_float(\n",
    "        tf.equal(predictions, tf.reduce_max(predictions, 1, keep_dims=True)))\n",
    "    y = y / tf.reduce_sum(y, 1, keep_dims=True)\n",
    "    loss = model_loss(y, predictions, mean=False)\n",
    "\n",
    "    # Define gradient of loss wrt input\n",
    "    grad, = tf.gradients(loss, x)\n",
    "\n",
    "    # Take sign of gradient\n",
    "    signed_grad = tf.sign(grad)\n",
    "\n",
    "    # Multiply by constant epsilon\n",
    "    scaled_signed_grad = eps * signed_grad\n",
    "\n",
    "    # Add perturbation to original example to obtain adversarial example\n",
    "    adv_x = tf.stop_gradient(x + scaled_signed_grad)\n",
    "\n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) and (clip_max is not None):\n",
    "        adv_x = tf.clip_by_value(adv_x, clip_min, clip_max)\n",
    "\n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import keras\n",
    "from keras import backend\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import app\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('nb_epochs', 6, 'Number of epochs to train model')\n",
    "flags.DEFINE_integer('batch_size', 128, 'Size of training batches')\n",
    "flags.DEFINE_float('learning_rate', 0.1, 'Learning rate for training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set TF random seed to improve reproducibility\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not hasattr(backend, \"tf\"):\n",
    "    raise RuntimeError(\"This tutorial requires keras to be configured\" \n",
    "                           \" to use the TensorFlow backend.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions ordering should follow the Theano convention\n",
    "if keras.backend.image_dim_ordering() != 'tf':\n",
    "    keras.backend.set_image_dim_ordering('tf')\n",
    "    print(\"INFO: '~/.keras/keras.json' sets 'image_dim_ordering' to \"\n",
    "            \"'th', temporarily setting to 'tf'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create TF session and set as Keras backend session\n",
    "sess = tf.Session()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Get MNIST test data\n",
    "X_train, Y_train, X_test, Y_test = data_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert Y_train.shape[1] == 10.\n",
    "label_smooth = .1\n",
    "Y_train = Y_train.clip(label_smooth / 9., 1. - label_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01111111,  0.01111111,  0.01111111,  0.01111111,  0.01111111,\n",
       "         0.9       ,  0.01111111,  0.01111111,  0.01111111,  0.01111111],\n",
       "       [ 0.9       ,  0.01111111,  0.01111111,  0.01111111,  0.01111111,\n",
       "         0.01111111,  0.01111111,  0.01111111,  0.01111111,  0.01111111]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input TF placeholder\n",
    "x = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined TensorFlow model graph.\n"
     ]
    }
   ],
   "source": [
    "# Define TF model graph\n",
    "model = cnn_model()\n",
    "predictions = model(x)\n",
    "print(\"Defined TensorFlow model graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # Evaluate the accuracy of the MNIST model on legitimate test examples\n",
    "    eval_params = {'batch_size': FLAGS.batch_size}\n",
    "    accuracy = model_eval(sess, x, y, predictions, X_test, Y_test,\n",
    "                          args=eval_params)\n",
    "    assert X_test.shape[0] == 10000, X_test.shape\n",
    "    print('Test accuracy on legitimate test examples: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tEpoch took 44.010509729385376 seconds\n",
      "Test accuracy on legitimate test examples: 0.9244\n",
      "Epoch 1\n",
      "\tEpoch took 43.45127630233765 seconds\n",
      "Test accuracy on legitimate test examples: 0.9479\n",
      "Epoch 2\n",
      "\tEpoch took 43.71137714385986 seconds\n",
      "Test accuracy on legitimate test examples: 0.9586\n",
      "Epoch 3\n",
      "\tEpoch took 43.88304424285889 seconds\n",
      "Test accuracy on legitimate test examples: 0.9645\n",
      "Epoch 4\n",
      "\tEpoch took 46.80714440345764 seconds\n",
      "Test accuracy on legitimate test examples: 0.9703\n",
      "Epoch 5\n",
      "\tEpoch took 45.878817558288574 seconds\n",
      "Test accuracy on legitimate test examples: 0.9727\n",
      "Completed model training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an MNIST model\n",
    "train_params = {\n",
    "    'nb_epochs': FLAGS.nb_epochs,\n",
    "    'batch_size': FLAGS.batch_size,\n",
    "    'learning_rate': FLAGS.learning_rate\n",
    "}\n",
    "\n",
    "model_train(sess, x, y, predictions, X_train, Y_train, evaluate=evaluate, args=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craft adversarial examples using Fast Gradient Sign Method (FGSM)\n",
    "adv_x = fgsm(x, predictions, eps=0.3)\n",
    "eval_params = {'batch_size': FLAGS.batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_adv, = batch_eval(sess, [x], [adv_x], [X_test], args=eval_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.02941176],\n",
       "         [ 0.4254902 ],\n",
       "         [ 0.32352942],\n",
       "         [ 0.29215688],\n",
       "         [-0.06470589],\n",
       "         [-0.15882353],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.57058823],\n",
       "         [ 0.69607842],\n",
       "         [ 0.69607842],\n",
       "         [ 0.69607842],\n",
       "         [ 0.69607842],\n",
       "         [ 0.64509803],\n",
       "         [ 0.47647059],\n",
       "         [ 0.47647059],\n",
       "         [ 0.47647059],\n",
       "         [ 0.47647059],\n",
       "         [ 0.47647059],\n",
       "         [ 0.47647059],\n",
       "         [ 0.47647059],\n",
       "         [ 0.47647059],\n",
       "         [ 0.36666667],\n",
       "         [-0.09607844],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.56274509],\n",
       "         [ 0.74705887],\n",
       "         [-0.01764706],\n",
       "         [ 0.14705881],\n",
       "         [ 0.3392157 ],\n",
       "         [ 0.59019607],\n",
       "         [ 0.69607842],\n",
       "         [ 0.58235294],\n",
       "         [ 0.69607842],\n",
       "         [ 0.69607842],\n",
       "         [ 0.69607842],\n",
       "         [ 0.68039215],\n",
       "         [ 0.59803921],\n",
       "         [ 0.69607842],\n",
       "         [ 0.69607842],\n",
       "         [ 0.24901962],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.23333335],\n",
       "         [-0.04117647],\n",
       "         [-0.24509805],\n",
       "         [-0.0372549 ],\n",
       "         [-0.0372549 ],\n",
       "         [-0.0372549 ],\n",
       "         [-0.06862746],\n",
       "         [-0.21764708],\n",
       "         [ 0.62549019],\n",
       "         [ 0.69607842],\n",
       "         [ 0.71568632],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.02549019],\n",
       "         [ 0.69215685],\n",
       "         [ 1.11960793],\n",
       "         [ 0.37058824],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.38627452],\n",
       "         [ 0.61372548],\n",
       "         [ 1.29999995],\n",
       "         [ 0.62549019],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.20588237],\n",
       "         [ 0.69607842],\n",
       "         [ 0.63333333],\n",
       "         [ 0.47254902],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.06862746],\n",
       "         [ 0.67647058],\n",
       "         [ 0.69607842],\n",
       "         [-0.05686276],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.22156864],\n",
       "         [ 0.69607842],\n",
       "         [ 0.43333334],\n",
       "         [-0.28039217],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.2647059 ],\n",
       "         [ 0.50392157],\n",
       "         [ 0.67254901],\n",
       "         [-0.07254903],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.19411764],\n",
       "         [ 0.69607842],\n",
       "         [ 0.4137255 ],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.00588235],\n",
       "         [ 0.68431371],\n",
       "         [ 0.64117646],\n",
       "         [ 0.52352941],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.22549021],\n",
       "         [ 0.56666666],\n",
       "         [ 0.69607842],\n",
       "         [ 0.95098042],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.31176472],\n",
       "         [ 0.49607843],\n",
       "         [ 0.69607842],\n",
       "         [ 1.15882349],\n",
       "         [ 0.43725491],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001]],\n",
       "\n",
       "        [[ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.44901961],\n",
       "         [ 0.69607842],\n",
       "         [ 0.69607842],\n",
       "         [ 0.60196078],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001]],\n",
       "\n",
       "        [[ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.42156863],\n",
       "         [ 1.17843139],\n",
       "         [ 0.69607842],\n",
       "         [ 0.75098038],\n",
       "         [ 0.30392158],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.82156867],\n",
       "         [ 1.29607844],\n",
       "         [ 1.29607844],\n",
       "         [ 0.50392157],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.53921568],\n",
       "         [ 1.24901962],\n",
       "         [ 1.29607844],\n",
       "         [ 0.69607842],\n",
       "         [-0.09607844],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.30000001],\n",
       "         [ 0.17450979],\n",
       "         [ 0.69607842],\n",
       "         [ 0.69607842],\n",
       "         [ 0.55882353],\n",
       "         [-0.14313726],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [ 0.17450979],\n",
       "         [ 0.69607842],\n",
       "         [ 0.51176471],\n",
       "         [-0.22941178],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]],\n",
       "\n",
       "        [[-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001],\n",
       "         [-0.30000001]]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_adv[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.32941177],\n",
       "         [ 0.72549021],\n",
       "         [ 0.62352943],\n",
       "         [ 0.59215689],\n",
       "         [ 0.23529412],\n",
       "         [ 0.14117648],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.87058824],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.94509804],\n",
       "         [ 0.7764706 ],\n",
       "         [ 0.7764706 ],\n",
       "         [ 0.7764706 ],\n",
       "         [ 0.7764706 ],\n",
       "         [ 0.7764706 ],\n",
       "         [ 0.7764706 ],\n",
       "         [ 0.7764706 ],\n",
       "         [ 0.7764706 ],\n",
       "         [ 0.66666669],\n",
       "         [ 0.20392157],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.26274511],\n",
       "         [ 0.44705883],\n",
       "         [ 0.28235295],\n",
       "         [ 0.44705883],\n",
       "         [ 0.63921571],\n",
       "         [ 0.89019608],\n",
       "         [ 0.99607843],\n",
       "         [ 0.88235295],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.98039216],\n",
       "         [ 0.89803922],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.54901963],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.06666667],\n",
       "         [ 0.25882354],\n",
       "         [ 0.05490196],\n",
       "         [ 0.26274511],\n",
       "         [ 0.26274511],\n",
       "         [ 0.26274511],\n",
       "         [ 0.23137255],\n",
       "         [ 0.08235294],\n",
       "         [ 0.9254902 ],\n",
       "         [ 0.99607843],\n",
       "         [ 0.41568628],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.32549021],\n",
       "         [ 0.99215686],\n",
       "         [ 0.81960785],\n",
       "         [ 0.07058824],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.08627451],\n",
       "         [ 0.9137255 ],\n",
       "         [ 1.        ],\n",
       "         [ 0.32549021],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.50588238],\n",
       "         [ 0.99607843],\n",
       "         [ 0.93333334],\n",
       "         [ 0.17254902],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.23137255],\n",
       "         [ 0.97647059],\n",
       "         [ 0.99607843],\n",
       "         [ 0.24313726],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.52156866],\n",
       "         [ 0.99607843],\n",
       "         [ 0.73333335],\n",
       "         [ 0.01960784],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.03529412],\n",
       "         [ 0.80392158],\n",
       "         [ 0.97254902],\n",
       "         [ 0.22745098],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.49411765],\n",
       "         [ 0.99607843],\n",
       "         [ 0.71372551],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.29411766],\n",
       "         [ 0.98431373],\n",
       "         [ 0.94117647],\n",
       "         [ 0.22352941],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.07450981],\n",
       "         [ 0.86666667],\n",
       "         [ 0.99607843],\n",
       "         [ 0.65098041],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.01176471],\n",
       "         [ 0.79607844],\n",
       "         [ 0.99607843],\n",
       "         [ 0.85882354],\n",
       "         [ 0.13725491],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.14901961],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.3019608 ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.12156863],\n",
       "         [ 0.87843138],\n",
       "         [ 0.99607843],\n",
       "         [ 0.4509804 ],\n",
       "         [ 0.00392157],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.52156866],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.20392157],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.23921569],\n",
       "         [ 0.94901961],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.20392157],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.47450981],\n",
       "         [ 0.99607843],\n",
       "         [ 0.99607843],\n",
       "         [ 0.85882354],\n",
       "         [ 0.15686275],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.47450981],\n",
       "         [ 0.99607843],\n",
       "         [ 0.81176472],\n",
       "         [ 0.07058824],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.91850078]\n",
      " [ 0.93933558]\n",
      " [ 0.89378422]\n",
      " ..., \n",
      " [ 0.93386763]\n",
      " [ 0.93458962]\n",
      " [ 0.93732846]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_max(predictions.eval({x:X_test, keras.backend.learning_phase():0}, session=sess), 1, keep_dims=True).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.16804680e-02   1.00518307e-02   1.16472468e-02 ...,   9.18500781e-01\n",
      "    3.87454638e-03   8.72335583e-03]\n",
      " [  1.03251608e-02   1.17031988e-02   9.39335585e-01 ...,   1.40797033e-03\n",
      "    5.79976942e-03   3.27410223e-03]\n",
      " [  1.38113210e-02   8.93784225e-01   9.21671465e-03 ...,   1.35107990e-02\n",
      "    7.43264705e-03   8.18998646e-03]\n",
      " ..., \n",
      " [  5.21284202e-03   5.76444110e-03   3.11589311e-03 ...,   7.52498815e-03\n",
      "    6.98998058e-03   2.63973009e-02]\n",
      " [  3.49080935e-03   1.54837698e-03   8.96481914e-04 ...,   6.61458168e-03\n",
      "    2.76952405e-02   2.81033269e-03]\n",
      " [  9.17047542e-03   8.61044507e-03   8.01769365e-03 ...,   5.21927467e-03\n",
      "    3.09171504e-03   4.95432364e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions.eval({x:X_test, keras.backend.learning_phase():0}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on adversarial examples: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the MNIST model on adversarial examples\n",
    "accuracy = model_eval(sess, x, y, predictions, np.array([X_test_adv[0]]), np.array([Y_test[0]]),\n",
    "                          args=eval_params)\n",
    "print('Test accuracy on adversarial examples: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03016986,  0.03341802,  0.24376959,  0.14268301,  0.03944839,\n",
       "         0.06488139,  0.01809699,  0.01743182,  0.37487182,  0.03522912]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.eval({x:np.array([X_test_adv[0]]), keras.backend.learning_phase():0}, session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc4f4056c18>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4f4138438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(X_test[0]).reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc4d6d9f7b8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4f4034b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(X_test_adv[0]).reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on adversarial examples: 0.0217\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the MNIST model on adversarial examples\n",
    "accuracy = model_eval(sess, x, y, predictions, X_test_adv, Y_test,\n",
    "                          args=eval_params)\n",
    "print('Test accuracy on adversarial examples: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeating the process, using adversarial training\n"
     ]
    }
   ],
   "source": [
    "print(\"Repeating the process, using adversarial training\")\n",
    "# Redefine TF model graph\n",
    "model_2 = cnn_model()\n",
    "predictions_2 = model_2(x)\n",
    "adv_x_2 = fgsm(x, predictions_2, eps=0.3)\n",
    "predictions_2_adv = model_2(adv_x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_2():\n",
    "        # Evaluate the accuracy of the adversarialy trained MNIST model on\n",
    "        # legitimate test examples\n",
    "        eval_params = {'batch_size': FLAGS.batch_size}\n",
    "        accuracy = model_eval(sess, x, y, predictions_2, X_test, Y_test,\n",
    "                              args=eval_params)\n",
    "        print('Test accuracy on legitimate test examples: ' + str(accuracy))\n",
    "\n",
    "        # Evaluate the accuracy of the adversarially trained MNIST model on\n",
    "        # adversarial examples\n",
    "        accuracy_adv = model_eval(sess, x, y, predictions_2_adv, X_test,\n",
    "                                  Y_test, args=eval_params)\n",
    "        print('Test accuracy on adversarial examples: ' + str(accuracy_adv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tEpoch took 100.55715894699097 seconds\n",
      "Test accuracy on legitimate test examples: 0.9233\n",
      "Test accuracy on adversarial examples: 0.2316\n",
      "Epoch 1\n",
      "\tEpoch took 100.49397683143616 seconds\n",
      "Test accuracy on legitimate test examples: 0.952\n",
      "Test accuracy on adversarial examples: 0.347\n",
      "Epoch 2\n",
      "\tEpoch took 100.8620274066925 seconds\n",
      "Test accuracy on legitimate test examples: 0.9602\n",
      "Test accuracy on adversarial examples: 0.3996\n",
      "Epoch 3\n",
      "\tEpoch took 100.36771154403687 seconds\n",
      "Test accuracy on legitimate test examples: 0.9639\n",
      "Test accuracy on adversarial examples: 0.4343\n",
      "Epoch 4\n",
      "\tEpoch took 100.47701644897461 seconds\n",
      "Test accuracy on legitimate test examples: 0.9672\n",
      "Test accuracy on adversarial examples: 0.4687\n",
      "Epoch 5\n",
      "\tEpoch took 100.33534026145935 seconds\n",
      "Test accuracy on legitimate test examples: 0.9695\n",
      "Test accuracy on adversarial examples: 0.5056\n",
      "Epoch 6\n",
      "\tEpoch took 101.85268974304199 seconds\n",
      "Test accuracy on legitimate test examples: 0.9722\n",
      "Test accuracy on adversarial examples: 0.5431\n",
      "Epoch 7\n",
      "\tEpoch took 101.5665431022644 seconds\n",
      "Test accuracy on legitimate test examples: 0.9734\n",
      "Test accuracy on adversarial examples: 0.5565\n",
      "Epoch 8\n",
      "\tEpoch took 103.94926953315735 seconds\n",
      "Test accuracy on legitimate test examples: 0.9747\n",
      "Test accuracy on adversarial examples: 0.5939\n",
      "Epoch 9\n",
      "\tEpoch took 102.99558639526367 seconds\n",
      "Test accuracy on legitimate test examples: 0.9761\n",
      "Test accuracy on adversarial examples: 0.617\n",
      "Completed model training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an MNIST model\n",
    "train_params = {\n",
    "    'nb_epochs': 10,\n",
    "    'batch_size': FLAGS.batch_size,\n",
    "    'learning_rate': FLAGS.learning_rate\n",
    "}\n",
    "\n",
    "    \n",
    "# Perform adversarial training\n",
    "model_train(sess, x, y, predictions_2, X_train, Y_train,\n",
    "            predictions_adv=predictions_2_adv, evaluate=evaluate_2,\n",
    "            args=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_adv_2, = batch_eval(sess, [x], [adv_x_2], [X_test], args=eval_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc47dde9ac8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQhB8I6v8A8ROZfVGqMaAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4c46e7ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test_adv_2[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on adversarial examples: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the MNIST model on adversarial examples\n",
    "accuracy = model_eval(sess, x, y, predictions_2_adv, np.array([X_test_adv_2[0]]), np.array([Y_test[0]]),\n",
    "                          args=eval_params)\n",
    "print('Test accuracy on adversarial examples: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02298293,  0.02284614,  0.01305325,  0.19911435,  0.02017662,\n",
       "         0.62100452,  0.01645812,  0.02117987,  0.03327766,  0.02990649]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_2_adv.eval({x:np.array([X_test_adv_2[0]]),keras.backend.learning_phase():0}, session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on adversarial examples: 0.1305\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the MNIST model on adversarial examples\n",
    "accuracy = model_eval(sess, x, y, predictions_2_adv, X_test_adv_2, Y_test,\n",
    "                          args=eval_params)\n",
    "print('Test accuracy on adversarial examples: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.",
   "language": "python",
   "name": "python3."
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
